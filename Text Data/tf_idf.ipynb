{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f436d647-b24a-4d54-bb09-2a7a0062125e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "from scipy.spatial.distance import minkowski, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada0715-5474-45f3-b0b5-98d167a53ee2",
   "metadata": {},
   "source": [
    "# Textual Data \n",
    "\n",
    "This project includes many steps of data processing\n",
    "1. First the data is structured so that it can be easily processed with fewer memory\n",
    "    - Only the title data will be used\n",
    "2. The documents will be processed in order to generate their TF-IDF representation \n",
    "3. The SVD representation will be generated in order to compress the space used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bc8ce-c801-4a58-9b1c-34f64a406573",
   "metadata": {},
   "source": [
    "### Data Structuring\n",
    "\n",
    "Chaging the tabled format of the data into an hashing structure that contains all the words and a list of sets with the words present in each document. <br>\n",
    "Punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69052c12-bf89-4bca-9427-adcd6823981d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"data/arxiv_data.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0912a-7de6-4166-92ee-451c44bf3faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = pd.read_csv(\"data/arxiv_data.csv\").titles#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce715b5-f001-4048-b039-5a90abaeca55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(document):\n",
    "    for word in nltk.tokenize.word_tokenize(document):\n",
    "        if word not in string.punctuation:\n",
    "            yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e4af9-2210-4ceb-8185-e7d456fe8773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "docs = []\n",
    "index = 0\n",
    "for t in titles:\n",
    "    current_doc = []\n",
    "    for word in get_words(t):\n",
    "        if word not in all_words:\n",
    "            all_words|= {word:index}\n",
    "            index+=1\n",
    "        current_doc.append(word)\n",
    "    docs.append(current_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea08b3-aad6-4130-9566-9ea07ff227a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/arxiv_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((all_words, docs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b3e14b6-56b6-44b9-a64d-3e5532afe93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25299, 51774)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9527e8-a9fd-4123-86f0-be65334959a6",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "The text data is transformed into TF-IDF structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578acc3c-bcbd-4d11-ab7d-0027f14030b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_words, docs = pickle.load(open(\"data/arxiv_data.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19e7780-c4e4-4c22-b6b7-9c4291a6e4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(words: list[str]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Counts the ocurrence of each word in the document corpus.\n",
    "    \"\"\"\n",
    "    return dict(zip(*np.unique(words, return_counts=True)))\n",
    "    \n",
    "    \n",
    "def tf(word_counts: dict, i: int, TF_Matrix) -> None:\n",
    "    \"\"\"\n",
    "    Computes the Term-Frequency vector of a document. \n",
    "    Operates in-memory in the Term-Frequency Matrix, receiving the index {i} that corresponds to the document.\n",
    "    \"\"\"\n",
    "    counts = word_counts.values()\n",
    "    if len(counts)==0: return {}\n",
    "    max_value = max(counts)\n",
    "    \n",
    "    for word, counts in word_counts.items():\n",
    "        TF_Matrix[i, all_words[word]] = counts/max_value\n",
    "    \n",
    "    \n",
    "def calc_sparse_tf_matrix(docs: list[list[str]], TF_Matrix):\n",
    "    \"\"\"\n",
    "    Computes the Term-Frequency Matrix\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        word_counts = count_words(doc)\n",
    "        tf(word_counts, i, TF_Matrix)\n",
    "    return TF_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af77a8cd-3582-459b-8f9f-237282a9c36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_idf(documents, IDF_Matrix):\n",
    "    \"\"\"\n",
    "    Computes the Inverse Document Frequency in-memory.\n",
    "    \"\"\"\n",
    "    N = len(documents)\n",
    "    \n",
    "    for doc in documents:\n",
    "        for word in np.unique(doc):\n",
    "            IDF_Matrix[ 0, all_words[word] ] += 1\n",
    "    \n",
    "    for i in range(IDF_Matrix.shape[1]):\n",
    "        IDF_Matrix[0, i] = np.log2(N / IDF_Matrix[0, i] )\n",
    "    return IDF_Matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acac3d01-fda9-48de-afec-e243bc8576ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_tf_idf(all_words: dict[str, int],\n",
    "                documents: list[list[str]]):\n",
    "    \n",
    "    TF = dok_matrix(np.zeros((len(docs), len(all_words))))\n",
    "    IDF = dok_matrix(np.zeros((1, len(all_words))))\n",
    "    \n",
    "    TF = calc_sparse_tf_matrix(documents, TF).tocsr()\n",
    "    IDF = calc_idf(documents, IDF).tocsr()\n",
    "    \n",
    "    print(\"TF size =\", sys.getsizeof(pickle.dumps(TF))/1024**2, \"MB\")\n",
    "    print(\"IDF size =\", sys.getsizeof(pickle.dumps(IDF))/1024**2, \"MB\")\n",
    "    \n",
    "    TF_IDF = TF.multiply(IDF)\n",
    "    return TF_IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5241385-6357-49cc-bb1e-2073710fd6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF size = 5.424110412597656 MB\n",
      "IDF size = 0.28991127014160156 MB\n"
     ]
    }
   ],
   "source": [
    "TF_IDF = calc_tf_idf(all_words, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890d943d-2ddb-4113-acdd-c9fe689e4e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.424110412597656"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(pickle.dumps(TF_IDF))/1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057ae26-c955-4c49-8981-e1eb894a382d",
   "metadata": {},
   "source": [
    "### From the sparse TF-IDF Matrix, create the Singular Decomposition Values\n",
    "\n",
    "The matrix is decomposed into R context values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1defcb10-b20b-419a-a737-6b4a54989119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "U, S, V = svds(TF_IDF, k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdf8a94-e1aa-4fcf-86dd-94ec12424bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51774, 30), (30,), (30, 25299))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, S.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda6f7c3-929a-4701-bea5-973080069750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compressed_docs = (np.diag(S).T @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79108514-7140-41a2-a2bf-c84153332c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "dists = [ cosine(\n",
    "                compressed_docs[:, k], \n",
    "                compressed_docs[:, i]\n",
    "            ) for i in range(compressed_docs.shape[1])\n",
    "        ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
