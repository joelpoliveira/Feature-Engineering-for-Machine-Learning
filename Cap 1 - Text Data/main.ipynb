{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fda414-a487-4de9-b80d-04b65e75b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from scipy.spatial.distance import minkowski, cosine\n",
    "from IPython.display import display\n",
    "from typing import Sequence, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d3a943-5fb4-4f7c-ac5f-43a729469ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"arxiv_data.csv\", nrows=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59927040-faec-4f4c-bd5e-f8437a657551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f624fd-3608-4d68-a2fb-45728f03a8b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be64b8d6-8afb-419d-99c2-220f7932c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(index):\n",
    "    return df.loc[index, \"summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65d3c9b-282d-412d-892e-8dd41e400eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(all_docs)->set:\n",
    "    vocab = set()\n",
    "    for doc in all_docs:\n",
    "        for word in nltk.word_tokenize(doc):\n",
    "            vocab|={word.lower()}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b87491f-f15c-47fd-bb55-64432d8c7126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7684"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(get_vocabulary(df[\"summaries\"]))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7141006-7e14-41d8-8a64-c231daac7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = lambda i: pd.Index(data = range(mat.shape[0])).difference([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f22134-26cf-4c9f-84a8-d11610b5493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(i: int):\n",
    "    not_i = exclude(i)\n",
    "    \n",
    "    most_sim = mat[i,not_i].argmin()\n",
    "    most_sim = not_i[most_sim]\n",
    "    \n",
    "    print(f\"Min. Dist. Index = {most_sim}\")\n",
    "    print(f\"Min. Dist. = {mat[i, most_sim]}\\n\")\n",
    "    print(get(i))\n",
    "    print()\n",
    "    print(get(most_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e042b35-8b00-42f0-83d2-a8328ab46c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(i: int, top=10):\n",
    "    not_i = exclude(i)\n",
    "    \n",
    "    candidates = mat[i,not_i].argsort()[1:top+1]\n",
    "    candidates = not_i[candidates]\n",
    "    \n",
    "    cand_scores = list(map(lambda x: float(f\"{x:.4f}\"), mat[i, candidates]))\n",
    "\n",
    "    print(f\"\"\"Top Candidates: {candidates}\n",
    "Their Scores: {cand_scores}\"\"\")\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50052f81-9aa6-42f0-b032-2f7e136e93c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Term Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8249217-486d-4d51-bb5a-47104bb2445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(document: str, vocab: list):\n",
    "    bow = np.zeros(len(vocab))\n",
    "    loc = dict(map(lambda pair: (pair[1], pair[0]), enumerate(vocab)))\n",
    "    \n",
    "    for word in nltk.word_tokenize(document):\n",
    "        bow[loc[word.lower()]]=1\n",
    "        \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b295cc9-ecd6-4798-a86d-a76b354fd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_mat(N, dist):\n",
    "    bows = [ bag_of_words(get(i), vocab) for i in range(N) ]\n",
    "    \n",
    "    sim_mat = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            sim_mat[i,j] = dist(bows[i], bows[j])\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    display(pd.DataFrame(data = sim_mat, columns=range(N), index=range(N)).loc[0:5, 0:5])\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d676be-334d-4b07-8eb1-4e20eb95c118",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bff8092e-f85e-4ca2-8360-f5ebe2295497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.266499</td>\n",
       "      <td>13.304135</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.638182</td>\n",
       "      <td>11.958261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.266499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.132746</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>15.556349</td>\n",
       "      <td>14.177447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.304135</td>\n",
       "      <td>15.132746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.490738</td>\n",
       "      <td>15.264338</td>\n",
       "      <td>13.490738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.730920</td>\n",
       "      <td>13.490738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.489996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.638182</td>\n",
       "      <td>15.556349</td>\n",
       "      <td>15.264338</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.035669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.958261</td>\n",
       "      <td>14.177447</td>\n",
       "      <td>13.490738</td>\n",
       "      <td>12.489996</td>\n",
       "      <td>14.035669</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5\n",
       "0   0.000000  13.266499  13.304135  13.000000  13.638182  11.958261\n",
       "1  13.266499   0.000000  15.132746  14.730920  15.556349  14.177447\n",
       "2  13.304135  15.132746   0.000000  13.490738  15.264338  13.490738\n",
       "3  13.000000  14.730920  13.490738   0.000000  15.000000  12.489996\n",
       "4  13.638182  15.556349  15.264338  15.000000   0.000000  14.035669\n",
       "5  11.958261  14.177447  13.490738  12.489996  14.035669   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2dbbef72-8aa1-4eb3-8a6e-ae177e439045",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a442070a-0085-4677-91b9-1487e4ca4a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 147\n",
      "Min. Dist. = 10.488088481701515\n",
      "\n",
      "In this work, we address the challenging task of few-shot segmentation.\n",
      "Previous few-shot segmentation methods mainly employ the information of support\n",
      "images as guidance for query image segmentation. Although some works propose to\n",
      "build cross-reference between support and query images, their extraction of\n",
      "query information still depends on the support images. We here propose to\n",
      "extract the information from the query itself independently to benefit the\n",
      "few-shot segmentation task. To this end, we first propose a prior extractor to\n",
      "learn the query information from the unlabeled images with our proposed\n",
      "global-local contrastive learning. Then, we extract a set of predetermined\n",
      "priors via this prior extractor. With the obtained priors, we generate the\n",
      "prior region maps for query images, which locate the objects, as guidance to\n",
      "perform cross interaction with support features. In such a way, the extraction\n",
      "of query information is detached from the support branch, overcoming the\n",
      "limitation by support, and could obtain more informative query clues to achieve\n",
      "better interaction. Without bells and whistles, the proposed approach achieves\n",
      "new state-of-the-art performance for the few-shot segmentation task on\n",
      "PASCAL-5$^{i}$ and COCO datasets.\n",
      "\n",
      "This paper presents a game, controlled by computer vision, in identification\n",
      "of hand gestures (hand-tracking). The proposed work is based on image\n",
      "segmentation and construction of a convex hull with Jarvis Algorithm , and\n",
      "determination of the pattern based on the extraction of area characteristics in\n",
      "the convex hull.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e8bc15ba-fafe-4881-a375-04d1215971cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([354, 19, 282, 472, 459, 491, 382, 35, 293, 173], dtype='int64')\n",
      "Their Scores: [10.9087, 11.1803, 11.225, 11.2694, 11.2694, 11.3578, 11.4018, 11.4018, 11.4455, 11.4891]\n"
     ]
    }
   ],
   "source": [
    "rete = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1da64-f454-43a1-b250-6b2377a91226",
   "metadata": {},
   "source": [
    "> For document indexed with *0*, altough both the abstracts are revolve around image, their main research topic are not alike. <br><br>\n",
    "This occurs for more than one document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b9fa3-4b13-41c7-ae78-44a5966c6ac4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bfe4d14d-3105-4024-89f0-d6b631c47fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780618</td>\n",
       "      <td>0.759312</td>\n",
       "      <td>0.790238</td>\n",
       "      <td>0.725305</td>\n",
       "      <td>0.756036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820732</td>\n",
       "      <td>0.831268</td>\n",
       "      <td>0.805819</td>\n",
       "      <td>0.846417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.759312</td>\n",
       "      <td>0.820732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678378</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.742052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.790238</td>\n",
       "      <td>0.831268</td>\n",
       "      <td>0.678378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776502</td>\n",
       "      <td>0.688504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.725305</td>\n",
       "      <td>0.805819</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.776502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.756036</td>\n",
       "      <td>0.846417</td>\n",
       "      <td>0.742052</td>\n",
       "      <td>0.688504</td>\n",
       "      <td>0.735822</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.000000  0.780618  0.759312  0.790238  0.725305  0.756036\n",
       "1  0.780618  0.000000  0.820732  0.831268  0.805819  0.846417\n",
       "2  0.759312  0.820732  0.000000  0.678378  0.758392  0.742052\n",
       "3  0.790238  0.831268  0.678378  0.000000  0.776502  0.688504\n",
       "4  0.725305  0.805819  0.758392  0.776502  0.000000  0.735822\n",
       "5  0.756036  0.846417  0.742052  0.688504  0.735822  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b564be5-78ca-4c91-9108-80f82582dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df6a991b-06e7-4bb3-b4cb-8d833849300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 19\n",
      "Min. Dist. = 0.6399588500884522\n",
      "\n",
      "In this work, we address the challenging task of few-shot segmentation.\n",
      "Previous few-shot segmentation methods mainly employ the information of support\n",
      "images as guidance for query image segmentation. Although some works propose to\n",
      "build cross-reference between support and query images, their extraction of\n",
      "query information still depends on the support images. We here propose to\n",
      "extract the information from the query itself independently to benefit the\n",
      "few-shot segmentation task. To this end, we first propose a prior extractor to\n",
      "learn the query information from the unlabeled images with our proposed\n",
      "global-local contrastive learning. Then, we extract a set of predetermined\n",
      "priors via this prior extractor. With the obtained priors, we generate the\n",
      "prior region maps for query images, which locate the objects, as guidance to\n",
      "perform cross interaction with support features. In such a way, the extraction\n",
      "of query information is detached from the support branch, overcoming the\n",
      "limitation by support, and could obtain more informative query clues to achieve\n",
      "better interaction. Without bells and whistles, the proposed approach achieves\n",
      "new state-of-the-art performance for the few-shot segmentation task on\n",
      "PASCAL-5$^{i}$ and COCO datasets.\n",
      "\n",
      "Image segmentation is a common and challenging task in autonomous driving.\n",
      "Availability of sufficient pixel-level annotations for the training data is a\n",
      "hurdle. Active learning helps learning from small amounts of data by suggesting\n",
      "the most promising samples for labeling. In this work, we propose a new\n",
      "pool-based method for active learning, which proposes promising patches\n",
      "extracted from full image, in each acquisition step. The problem is framed in\n",
      "an exploration-exploitation framework by combining an embedding based on\n",
      "Uniform Manifold Approximation to model representativeness with entropy as\n",
      "uncertainty measure to model informativeness. We applied our proposed method to\n",
      "the autonomous driving datasets CamVid and Cityscapes and performed a\n",
      "quantitative comparison with state-of-the-art baselines. We find that our\n",
      "active learning method achieves better performance compared to previous\n",
      "methods.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7da8169-cae9-4b4b-a49f-36062fa089a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([257, 173, 48, 379, 498, 166, 461, 355, 237, 205], dtype='int64')\n",
      "Their Scores: [0.6538, 0.6726, 0.6731, 0.6731, 0.6746, 0.6746, 0.6752, 0.6796, 0.6851, 0.6858]\n"
     ]
    }
   ],
   "source": [
    "rcte = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6319030-dec5-4751-a708-9b451c56f153",
   "metadata": {},
   "source": [
    "> Similarly to the last scenario, in *0*, both the papers talk about image processing. \n",
    "\n",
    "> In index *45* both papers refer image segmentation as the topic of the research, applied to different cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80ba6b-f820-494e-9f44-21491203c159",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dot Product Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32316402-3c99-4213-aebf-935be7ac80a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.011364  0.041667  0.037037  0.045455  0.030303  0.043478\n",
       "1  0.041667  0.007353  0.040000  0.045455  0.034483  0.055556\n",
       "2  0.037037  0.040000  0.006993  0.023256  0.027027  0.032258\n",
       "3  0.045455  0.045455  0.023256  0.008000  0.031250  0.028571\n",
       "4  0.030303  0.034483  0.027027  0.031250  0.006098  0.029412\n",
       "5  0.043478  0.055556  0.032258  0.028571  0.029412  0.009901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, lambda a,b: 1/np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95ecaf27-2d75-4d31-a05f-78b691d0ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a0c8103-be34-434e-a995-d69982e46f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 41\n",
      "Min. Dist. = 0.024390243902439025\n",
      "\n",
      "In this work, we address the challenging task of few-shot segmentation.\n",
      "Previous few-shot segmentation methods mainly employ the information of support\n",
      "images as guidance for query image segmentation. Although some works propose to\n",
      "build cross-reference between support and query images, their extraction of\n",
      "query information still depends on the support images. We here propose to\n",
      "extract the information from the query itself independently to benefit the\n",
      "few-shot segmentation task. To this end, we first propose a prior extractor to\n",
      "learn the query information from the unlabeled images with our proposed\n",
      "global-local contrastive learning. Then, we extract a set of predetermined\n",
      "priors via this prior extractor. With the obtained priors, we generate the\n",
      "prior region maps for query images, which locate the objects, as guidance to\n",
      "perform cross interaction with support features. In such a way, the extraction\n",
      "of query information is detached from the support branch, overcoming the\n",
      "limitation by support, and could obtain more informative query clues to achieve\n",
      "better interaction. Without bells and whistles, the proposed approach achieves\n",
      "new state-of-the-art performance for the few-shot segmentation task on\n",
      "PASCAL-5$^{i}$ and COCO datasets.\n",
      "\n",
      "Automated segmentation in medical image analysis is a challenging task that\n",
      "requires a large amount of manually labeled data. However, most existing\n",
      "learning-based approaches usually suffer from limited manually annotated\n",
      "medical data, which poses a major practical problem for accurate and robust\n",
      "medical image segmentation. In addition, most existing semi-supervised\n",
      "approaches are usually not robust compared with the supervised counterparts,\n",
      "and also lack explicit modeling of geometric structure and semantic\n",
      "information, both of which limit the segmentation accuracy. In this work, we\n",
      "present SimCVD, a simple contrastive distillation framework that significantly\n",
      "advances state-of-the-art voxel-wise representation learning. We first describe\n",
      "an unsupervised training strategy, which takes two views of an input volume and\n",
      "predicts their signed distance maps of object boundaries in a contrastive\n",
      "objective, with only two independent dropout as mask. This simple approach\n",
      "works surprisingly well, performing on the same level as previous fully\n",
      "supervised methods with much less labeled data. We hypothesize that dropout can\n",
      "be viewed as a minimal form of data augmentation and makes the network robust\n",
      "to representation collapse. Then, we propose to perform structural distillation\n",
      "by distilling pair-wise similarities. We evaluate SimCVD on two popular\n",
      "datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\n",
      "dataset. The results on the LA dataset demonstrate that, in two types of\n",
      "labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n",
      "90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\n",
      "previous best results. Our method can be trained in an end-to-end fashion,\n",
      "showing the promise of utilizing SimCVD as a general framework for downstream\n",
      "tasks, such as medical image synthesis and registration.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74d244fc-b389-4ef1-b6a4-4d7f37d87a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([355, 452, 184, 203, 257, 113, 122, 485, 205, 177], dtype='int64')\n",
      "Their Scores: [0.0244, 0.0256, 0.0263, 0.0263, 0.0278, 0.0278, 0.0278, 0.0278, 0.0278, 0.0278]\n"
     ]
    }
   ],
   "source": [
    "rdte = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f6047-f114-4616-a2a6-40f2dcfc0b35",
   "metadata": {},
   "source": [
    "> For index *0*, same result as the *cosine* scenario.\n",
    "\n",
    "> In index *45* both papers refer image segmentation as the topic of the research, applied to different cases. <br>\n",
    "\\* Not the same document as in *cosine* distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ce10d02-1ad2-4f2e-807b-d5a00d88b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([354, 19, 282, 472, 459, 491, 382, 35, 293, 173], dtype='int64')\n",
      "Int64Index([257, 173, 48, 379, 498, 166, 461, 355, 237, 205], dtype='int64')\n",
      "Int64Index([355, 452, 184, 203, 257, 113, 122, 485, 205, 177], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(rete, rcte, rdte, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dfc92d92-461f-4b29-a4eb-bdc114c24e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rete, rcte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56a4b23f-a3f8-4029-aa99-84c19cf520f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rete, rdte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e7094bc-4509-4634-a504-cbfcb77ee6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([205, 257, 355], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rdte, rcte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15985b-5cac-45ce-9e0f-bb63c1e8b30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "> ## Final comments\n",
    "\n",
    "We can see that the different distance methods recommend majorly distinct documents. This could mean that this Bagging method is not sufficiently good to represent the data, as even with difference metrics, the intersection set would be expected to be bigger then what it was.\n",
    "\n",
    "**The removal of stopwords could improve this model results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b672-560f-42a4-b1d8-d87250730666",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1f61ac8-4be4-4123-9619-c4a8a658118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(document: str, vocab: list):\n",
    "    bow = np.zeros(len(vocab))\n",
    "    loc = dict(map(lambda pair: (pair[1], pair[0]), enumerate(vocab)))\n",
    "    \n",
    "    for word in nltk.word_tokenize(document):\n",
    "        bow[loc[word.lower()]]+=1\n",
    "        \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6a32fb3-2c47-47b3-92c5-83d589a7c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_mat(N, dist):\n",
    "    bows = [ bag_of_words(get(i), vocab) for i in range(N) ]\n",
    "    \n",
    "    sim_mat = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            sim_mat[i,j] = dist(bows[i], bows[j])\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    display(pd.DataFrame(data = sim_mat, columns=range(N), index=range(N)).loc[0:5, 0:5])\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40d07e-8b6f-476f-aa92-9d48099a3851",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7ae92a9-a8b4-47e1-ba1e-479f4df28ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.764760</td>\n",
       "      <td>25.806976</td>\n",
       "      <td>23.811762</td>\n",
       "      <td>33.970576</td>\n",
       "      <td>18.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.764760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.151169</td>\n",
       "      <td>33.196385</td>\n",
       "      <td>32.326460</td>\n",
       "      <td>29.832868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.806976</td>\n",
       "      <td>33.151169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.712315</td>\n",
       "      <td>30.659419</td>\n",
       "      <td>23.811762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.811762</td>\n",
       "      <td>33.196385</td>\n",
       "      <td>20.712315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.930952</td>\n",
       "      <td>22.226111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.970576</td>\n",
       "      <td>32.326460</td>\n",
       "      <td>30.659419</td>\n",
       "      <td>28.930952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.921779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.894444</td>\n",
       "      <td>29.832868</td>\n",
       "      <td>23.811762</td>\n",
       "      <td>22.226111</td>\n",
       "      <td>31.921779</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5\n",
       "0   0.000000  31.764760  25.806976  23.811762  33.970576  18.894444\n",
       "1  31.764760   0.000000  33.151169  33.196385  32.326460  29.832868\n",
       "2  25.806976  33.151169   0.000000  20.712315  30.659419  23.811762\n",
       "3  23.811762  33.196385  20.712315   0.000000  28.930952  22.226111\n",
       "4  33.970576  32.326460  30.659419  28.930952   0.000000  31.921779\n",
       "5  18.894444  29.832868  23.811762  22.226111  31.921779   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, minkowski)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b47d638-d0f0-4e04-8ac5-801c238b06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 35\n",
      "Min. Dist. = 16.73320053068151\n",
      "\n",
      "Image segmentation has long been a basic problem in computer vision.\n",
      "Depth-wise Layering is a kind of segmentation that slices an image in a\n",
      "depth-wise sequence unlike the conventional image segmentation problems dealing\n",
      "with surface-wise decomposition. The proposed Depth-wise Layering technique\n",
      "uses a single depth image of a static scene to slice it into multiple layers.\n",
      "The technique employs a thresholding approach to segment rows of the dense\n",
      "depth map into smaller partitions called Line-Segments in this paper. Then, it\n",
      "uses the line-segment labelling method to identify number of objects and layers\n",
      "of the scene independently. The final stage is to link objects of the scene to\n",
      "their respective object-layers. We evaluate the efficiency of the proposed\n",
      "technique by applying that on many images along with their dense depth maps.\n",
      "The experiments have shown promising results of layering.\n",
      "\n",
      "Segmentation of images is a long-standing challenge in medical AI. This is\n",
      "mainly due to the fact that training a neural network to perform image\n",
      "segmentation requires a significant number of pixel-level annotated data, which\n",
      "is often unavailable. To address this issue, we propose a semi-supervised image\n",
      "segmentation technique based on the concept of multi-view learning. In contrast\n",
      "to the previous art, we introduce an adversarial form of dual-view training and\n",
      "employ a critic to formulate the learning problem in multi-view training as a\n",
      "min-max problem. Thorough quantitative and qualitative evaluations on several\n",
      "datasets indicate that our proposed method outperforms state-of-the-art medical\n",
      "image segmentation algorithms consistently and comfortably. The code is\n",
      "publicly available at https://github.com/himashi92/Duo-SegNet\n"
     ]
    }
   ],
   "source": [
    "index = 327\n",
    "\n",
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8a616cf-5f40-4fac-9078-88bf6e70767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([438, 200, 463, 205, 294, 166, 304, 478, 71, 383], dtype='int64')\n",
      "Their Scores: [19.6977, 20.9284, 21.2838, 21.587, 21.7486, 21.7945, 22.0454, 22.0907, 22.3159, 22.3159]\n"
     ]
    }
   ],
   "source": [
    "retw = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc495b8d-b14b-43aa-bd7c-ebb801786d00",
   "metadata": {},
   "source": [
    "> Index *0*: The documents don't really share anything beside the fact that are image related\n",
    "\n",
    "> Index *327*: Both Related to Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7132d-c0ba-427e-ab56-30d29b1195ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "47257255-6224-44c6-b189-59b8bca87ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478418</td>\n",
       "      <td>0.414020</td>\n",
       "      <td>0.407314</td>\n",
       "      <td>0.353476</td>\n",
       "      <td>0.414093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.478418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452115</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.312110</td>\n",
       "      <td>0.401459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.414020</td>\n",
       "      <td>0.452115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226834</td>\n",
       "      <td>0.292437</td>\n",
       "      <td>0.337706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407314</td>\n",
       "      <td>0.476195</td>\n",
       "      <td>0.226834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251733</td>\n",
       "      <td>0.343801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.353476</td>\n",
       "      <td>0.312110</td>\n",
       "      <td>0.292437</td>\n",
       "      <td>0.251733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.414093</td>\n",
       "      <td>0.401459</td>\n",
       "      <td>0.337706</td>\n",
       "      <td>0.343801</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.000000  0.478418  0.414020  0.407314  0.353476  0.414093\n",
       "1  0.478418  0.000000  0.452115  0.476195  0.312110  0.401459\n",
       "2  0.414020  0.452115  0.000000  0.226834  0.292437  0.337706\n",
       "3  0.407314  0.476195  0.226834  0.000000  0.251733  0.343801\n",
       "4  0.353476  0.312110  0.292437  0.251733  0.000000  0.287083\n",
       "5  0.414093  0.401459  0.337706  0.343801  0.287083  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, cosine)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5592e4e6-a326-441b-b8e6-2ca7e49fa825",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab52bf0c-96f0-4664-a1ec-708dc3ea375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 447\n",
      "Min. Dist. = 0.25660158929168186\n",
      "\n",
      "Image segmentation has long been a basic problem in computer vision.\n",
      "Depth-wise Layering is a kind of segmentation that slices an image in a\n",
      "depth-wise sequence unlike the conventional image segmentation problems dealing\n",
      "with surface-wise decomposition. The proposed Depth-wise Layering technique\n",
      "uses a single depth image of a static scene to slice it into multiple layers.\n",
      "The technique employs a thresholding approach to segment rows of the dense\n",
      "depth map into smaller partitions called Line-Segments in this paper. Then, it\n",
      "uses the line-segment labelling method to identify number of objects and layers\n",
      "of the scene independently. The final stage is to link objects of the scene to\n",
      "their respective object-layers. We evaluate the efficiency of the proposed\n",
      "technique by applying that on many images along with their dense depth maps.\n",
      "The experiments have shown promising results of layering.\n",
      "\n",
      "In machine learning and other fields, suggesting a good solution to a problem\n",
      "is usually a harder task than evaluating the quality of such a solution. This\n",
      "asymmetry is the basis for a large number of selection oriented methods that\n",
      "use a generator system to guess a set of solutions and an evaluator system to\n",
      "rank and select the best solutions. This work examines the use of this approach\n",
      "to the problem of panoptic image segmentation and class agnostic parts\n",
      "segmentation. The generator/evaluator approach for this case consists of two\n",
      "independent convolutional neural nets: a generator net that suggests variety\n",
      "segments corresponding to objects, stuff and parts regions in the image, and an\n",
      "evaluator net that chooses the best segments to be merged into the segmentation\n",
      "map. The result is a trial and error evolutionary approach in which a generator\n",
      "that guesses segments with low average accuracy, but with wide variability, can\n",
      "still produce good results when coupled with an accurate evaluator. The\n",
      "generator consists of a Pointer net that receives an image and a point in the\n",
      "image, and predicts the region of the segment containing the point. Generating\n",
      "and evaluating each segment separately is essential in this case since it\n",
      "demands exponentially fewer guesses compared to a system that guesses and\n",
      "evaluates the full segmentation map in each try. The classification of the\n",
      "selected segments is done by an independent region-specific classification net.\n",
      "This allows the segmentation to be class agnostic and hence, capable of\n",
      "segmenting unfamiliar categories that were not part of the training set. The\n",
      "method was examined on the COCO Panoptic segmentation benchmark and gave\n",
      "results comparable to those of the basic semantic segmentation and Mask-RCNN\n",
      "methods. In addition, the system was used for the task of splitting objects of\n",
      "unseen classes (that did not appear in the training set) into parts.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbe95c67-f19b-418e-95d2-d1f18ac7b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([83, 471, 488, 125, 235, 481, 399, 461, 111, 338], dtype='int64')\n",
      "Their Scores: [0.2692, 0.2792, 0.2909, 0.2947, 0.2956, 0.2977, 0.2986, 0.3041, 0.3051, 0.3054]\n"
     ]
    }
   ],
   "source": [
    "rctw = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552c5e7-fa08-4507-bc43-438476eb6184",
   "metadata": {},
   "source": [
    "> Index *0*: Both documents reference semantic image segmentation\n",
    "\n",
    "> Index *327*: Both have a repeated word \"segmentation\", but share different meaning, due to the context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690e2c5-1bd2-491b-a9c5-222e86df470c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dot Product Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6748f6e3-6a0e-43c1-9b8a-af25440d5817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.002242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.002427  0.002538  0.002646  0.002833  0.001757  0.003968\n",
       "1  0.002538  0.000722  0.001543  0.001748  0.000901  0.002119\n",
       "2  0.002646  0.001543  0.000990  0.001387  0.001026  0.002242\n",
       "3  0.002833  0.001748  0.001387  0.001161  0.001050  0.002451\n",
       "4  0.001757  0.000901  0.001026  0.001050  0.000532  0.001527\n",
       "5  0.003968  0.002119  0.002242  0.002451  0.001527  0.002227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, lambda a,b: 1/np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3a8cd7a7-308a-4779-b630-ae78bd7ba54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7237633c-f90e-4f71-ad02-aa83e303063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 235\n",
      "Min. Dist. = 0.001218026796589525\n",
      "\n",
      "Image segmentation has long been a basic problem in computer vision.\n",
      "Depth-wise Layering is a kind of segmentation that slices an image in a\n",
      "depth-wise sequence unlike the conventional image segmentation problems dealing\n",
      "with surface-wise decomposition. The proposed Depth-wise Layering technique\n",
      "uses a single depth image of a static scene to slice it into multiple layers.\n",
      "The technique employs a thresholding approach to segment rows of the dense\n",
      "depth map into smaller partitions called Line-Segments in this paper. Then, it\n",
      "uses the line-segment labelling method to identify number of objects and layers\n",
      "of the scene independently. The final stage is to link objects of the scene to\n",
      "their respective object-layers. We evaluate the efficiency of the proposed\n",
      "technique by applying that on many images along with their dense depth maps.\n",
      "The experiments have shown promising results of layering.\n",
      "\n",
      "The hatching process also influences the success of hatching eggs beside the\n",
      "initial egg factor. So that the results have a large percentage of hatching, it\n",
      "is necessary to check the development of the embryo at the beginning of the\n",
      "hatching. This process aims to sort eggs that have embryos to remain hatched\n",
      "until the end. Maximum checking is done the first week in the hatching period.\n",
      "This study aims to detect the presence of embryos in eggs. Detection of the\n",
      "existence of embryos is processed using segmentation. Egg images are segmented\n",
      "using the K-means algorithm based on Lab color images. The results of the\n",
      "images acquisition are converted into Lab color space images. The results of\n",
      "Lab color space images are processed using K-means for each color. The K-means\n",
      "process uses cluster k=3, where this cluster divided the image into three\n",
      "parts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs that\n",
      "have embryonic characteristics. This study applies the concept of color in the\n",
      "initial segmentation and grayscale in the final stages. The results of the\n",
      "initial phase show that the image segmentation results using k-means clustering\n",
      "based on Lab color space provide a grouping of three parts. At the grayscale\n",
      "image processing stage, the results of color image segmentation are processed\n",
      "with grayscaling, image enhancement, and morphology. Thus, it seems clear that\n",
      "the yolk segmented shows the presence of egg embryos. Based on this process and\n",
      "results, K-means segmentation based on Lab color space can be used for the\n",
      "initial stages of the embryo detection process. The evaluation uses MSE and\n",
      "MSSIM, with values of 0.0486 and 0.9979; this can be used as a reference that\n",
      "the results obtained can indicate the detection of embryos in egg yolk.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6e4545ee-f3d2-490f-8d50-76732069db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([447, 481, 125, 399, 338, 476, 83, 243, 4, 487], dtype='int64')\n",
      "Their Scores: [0.0013, 0.0013, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0017]\n"
     ]
    }
   ],
   "source": [
    "rdtw = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee7d2d-9491-4861-874b-382664e9358d",
   "metadata": {},
   "source": [
    "> Index *0*: Segmentation of Images\n",
    "\n",
    "> Index *327*: Segmentation of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e0b81-c1ff-4b29-bc21-40aa2de00d96",
   "metadata": {},
   "source": [
    "> ## Final Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cf6cafd0-7193-4321-842a-bdd0d6715699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([438, 200, 463, 205, 294, 166, 304, 478, 71, 383], dtype='int64')\n",
      "Int64Index([83, 471, 488, 125, 235, 481, 399, 461, 111, 338], dtype='int64')\n",
      "Int64Index([447, 481, 125, 399, 338, 476, 83, 243, 4, 487], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(retw, rctw, rdtw, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5620844-7cb9-4fe5-8396-6cb5f3a528fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(retw, rctw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "165f9bef-ef7d-4f54-b365-ec3277709a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(retw, rdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "07623197-0db5-4d8b-85e9-cb1b4c11a593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83, 125, 338, 399, 481], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rctw, rdtw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66819499-1ecd-451e-a345-638116c2a29a",
   "metadata": {},
   "source": [
    "**Cosine Distance** and **Dot Product Distance** share a great number of likely candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870efb64-7e7f-4838-9be3-9b2b9546eda3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Term Frequency Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8531d5c8-ca1a-4785-9292-e7938d013fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(document: str, vocab: list, t: Callable) -> Sequence:\n",
    "    bow = np.zeros(len(vocab))\n",
    "    loc = dict(map(lambda pair: (pair[1], pair[0]), enumerate(vocab)))\n",
    "    \n",
    "    for word in nltk.word_tokenize(document):\n",
    "        bow[loc[word.lower()]]+=1\n",
    "    \n",
    "    return t(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c7ae5f9b-c4b5-4484-b9d8-3a3ffe15dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_mat(N: int, dist: Callable, t: Callable):\n",
    "    bows = [ bag_of_words(get(i), vocab, t) for i in range(N) ]\n",
    "    \n",
    "    sim_mat = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            sim_mat[i,j] = dist(bows[i], bows[j])\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    display(pd.DataFrame(data = sim_mat, columns=range(N), index=range(N)).loc[0:5, 0:5])\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f29c7a3-49e4-40a9-8f30-000fe95f85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = lambda arr: np.log2(1+arr)\n",
    "t2 = lambda arr: np.log2(1 + t1(arr))\n",
    "t3 = lambda arr, k: (k+1)*arr/(arr+k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8061aa-9328-4b17-add2-4f7d9cb5ac9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Comparing effect of Different Frequency Transformers\n",
    "\n",
    "Given the fact that the book referenced vector dot product, that measure will be the one used for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "28f92c96-a676-4f8f-8ccd-64080e6eee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dist = lambda x, y: 1/np.dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0b071-4f01-4c49-bcc0-34d08cbfe044",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ee071cf8-9abc-4bea-bc8e-fb85b83a1dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.013929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.009425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.009618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.008678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.013929</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>0.005565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.006090  0.011734  0.010949  0.012623  0.009014  0.013929\n",
       "1  0.011734  0.003546  0.008557  0.010351  0.007956  0.012350\n",
       "2  0.010949  0.008557  0.003336  0.006837  0.006969  0.009425\n",
       "3  0.012623  0.010351  0.006837  0.004016  0.007510  0.009618\n",
       "4  0.009014  0.007956  0.006969  0.007510  0.002870  0.008678\n",
       "5  0.013929  0.012350  0.009425  0.009618  0.008678  0.005565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, dot_dist, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3847125c-bf30-4c20-a2dd-79e795878507",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "33a2bc2b-2670-4cf7-98a8-fee6767759a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 49\n",
      "Min. Dist. = 0.0072254638212528545\n",
      "\n",
      "The ability of neural networks to continuously learn and adapt to new tasks\n",
      "while retaining prior knowledge is crucial for many applications. However,\n",
      "current neural networks tend to forget previously learned tasks when trained on\n",
      "new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of\n",
      "Continual Learning (CL) is to alleviate this problem, which is particularly\n",
      "relevant for medical applications, where it may not be feasible to store and\n",
      "access previously used sensitive patient data. In this work, we propose a\n",
      "Continual Learning approach for brain segmentation, where a single network is\n",
      "consecutively trained on samples from different domains. We build upon an\n",
      "importance driven approach and adapt it for medical image segmentation.\n",
      "Particularly, we introduce learning rate regularization to prevent the loss of\n",
      "the network's knowledge. Our results demonstrate that directly restricting the\n",
      "adaptation of important network parameters clearly reduces Catastrophic\n",
      "Forgetting for segmentation across domains.\n",
      "\n",
      "Domain adaptation (DA) has drawn high interest for its capacity to adapt a\n",
      "model trained on labeled source data to perform well on unlabeled or weakly\n",
      "labeled target data from a different domain. Most common DA techniques require\n",
      "concurrent access to the input images of both the source and target domains.\n",
      "However, in practice, privacy concerns often impede the availability of source\n",
      "images in the adaptation phase. This is a very frequent DA scenario in medical\n",
      "imaging, where, for instance, the source and target images could come from\n",
      "different clinical sites. We introduce a source-free domain adaptation for\n",
      "image segmentation. Our formulation is based on minimizing a label-free entropy\n",
      "loss defined over target-domain data, which we further guide with a\n",
      "domain-invariant prior on the segmentation regions. Many priors can be derived\n",
      "from anatomical information. Here, a class ratio prior is estimated from\n",
      "anatomical knowledge and integrated in the form of a Kullback Leibler (KL)\n",
      "divergence in our overall loss function. Furthermore, we motivate our overall\n",
      "loss with an interesting link to maximizing the mutual information between the\n",
      "target images and their label predictions. We show the effectiveness of our\n",
      "prior aware entropy minimization in a variety of domain-adaptation scenarios,\n",
      "with different modalities and applications, including spine, prostate, and\n",
      "cardiac segmentation. Our method yields comparable results to several state of\n",
      "the art adaptation techniques, despite having access to much less information,\n",
      "as the source images are entirely absent in our adaptation phase. Our\n",
      "straightforward adaptation strategy uses only one network, contrary to popular\n",
      "adversarial techniques, which are not applicable to a source-free DA setting.\n",
      "Our framework can be readily used in a breadth of segmentation problems, and\n",
      "our code is publicly available: https://github.com/mathilde-b/SFDA\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3f764b56-1ae4-4fc3-83c9-65bce157b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([191, 14, 434, 222, 4, 425, 477, 78, 41, 432], dtype='int64')\n",
      "Their Scores: [0.0074, 0.0075, 0.0075, 0.0077, 0.0078, 0.0078, 0.0078, 0.0079, 0.008, 0.008]\n"
     ]
    }
   ],
   "source": [
    "rftt1 = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab25610-039f-479e-bb45-ca9cbf0e6c0a",
   "metadata": {},
   "source": [
    "> Index *0*: Image Segmentation\n",
    "\n",
    "> Index *440*: Both documents talk about adaptative models, the general topic seems to be highly related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78711d4b-97e0-4741-9402-5b30b1c5f3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Second Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3267d8bb-2783-482e-b069-b8d732619471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.022797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020499</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>0.019490</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.023296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.015909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022146</td>\n",
       "      <td>0.019490</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.015484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.015173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022797</td>\n",
       "      <td>0.023296</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.007332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.008158  0.020499  0.018565  0.022146  0.015655  0.022797\n",
       "1  0.020499  0.005135  0.016202  0.019490  0.015514  0.023296\n",
       "2  0.018565  0.016202  0.004714  0.011577  0.012807  0.015909\n",
       "3  0.022146  0.019490  0.011577  0.005561  0.014134  0.015484\n",
       "4  0.015655  0.015514  0.012807  0.014134  0.004192  0.015173\n",
       "5  0.022797  0.023296  0.015909  0.015484  0.015173  0.007332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, dot_dist, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3ff7866b-a5ee-4240-baf7-b0a05a396810",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c2464205-b848-4239-baec-cf3c4b542151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 191\n",
      "Min. Dist. = 0.01179637827596876\n",
      "\n",
      "The ability of neural networks to continuously learn and adapt to new tasks\n",
      "while retaining prior knowledge is crucial for many applications. However,\n",
      "current neural networks tend to forget previously learned tasks when trained on\n",
      "new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of\n",
      "Continual Learning (CL) is to alleviate this problem, which is particularly\n",
      "relevant for medical applications, where it may not be feasible to store and\n",
      "access previously used sensitive patient data. In this work, we propose a\n",
      "Continual Learning approach for brain segmentation, where a single network is\n",
      "consecutively trained on samples from different domains. We build upon an\n",
      "importance driven approach and adapt it for medical image segmentation.\n",
      "Particularly, we introduce learning rate regularization to prevent the loss of\n",
      "the network's knowledge. Our results demonstrate that directly restricting the\n",
      "adaptation of important network parameters clearly reduces Catastrophic\n",
      "Forgetting for segmentation across domains.\n",
      "\n",
      "Domain adaptation (DA) has drawn high interests for its capacity to adapt a\n",
      "model trained on labeled source data to perform well on unlabeled or weakly\n",
      "labeled target data from a different domain. Most common DA techniques require\n",
      "the concurrent access to the input images of both the source and target\n",
      "domains. However, in practice, it is common that the source images are not\n",
      "available in the adaptation phase. This is a very frequent DA scenario in\n",
      "medical imaging, for instance, when the source and target images come from\n",
      "different clinical sites. We propose a novel formulation for adapting\n",
      "segmentation networks, which relaxes such a constraint. Our formulation is\n",
      "based on minimizing a label-free entropy loss defined over target-domain data,\n",
      "which we further guide with a domain invariant prior on the segmentation\n",
      "regions. Many priors can be used, derived from anatomical information. Here, a\n",
      "class-ratio prior is learned via an auxiliary network and integrated in the\n",
      "form of a Kullback-Leibler (KL) divergence in our overall loss function. We\n",
      "show the effectiveness of our prior-aware entropy minimization in adapting\n",
      "spine segmentation across different MRI modalities. Our method yields\n",
      "comparable results to several state-of-the-art adaptation techniques, even\n",
      "though is has access to less information, the source images being absent in the\n",
      "adaptation phase. Our straight-forward adaptation strategy only uses one\n",
      "network, contrary to popular adversarial techniques, which cannot perform\n",
      "without the presence of the source images. Our framework can be readily used\n",
      "with various priors and segmentation problems.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dba5aa63-3451-4cd7-9805-1a04a13217d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([49, 14, 95, 222, 434, 41, 425, 477, 75, 78], dtype='int64')\n",
      "Their Scores: [0.0119, 0.0121, 0.0126, 0.0127, 0.0128, 0.0128, 0.0129, 0.013, 0.0131, 0.0132]\n"
     ]
    }
   ],
   "source": [
    "rftt2 = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad464ea-912e-43c7-ad41-e8922c991eb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Third Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a0af1871-6e37-4813-823e-8a938909a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "t3_k = lambda arr: t3(arr, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "da20401e-91ba-44cf-b041-29d8a9ef3847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.016773</td>\n",
       "      <td>0.015164</td>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.018714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016773</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.012843</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.018620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015164</td>\n",
       "      <td>0.012843</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.012950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>0.012884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.012606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.012606</td>\n",
       "      <td>0.006537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.007211  0.016773  0.015164  0.017970  0.012984  0.018714\n",
       "1  0.016773  0.004526  0.012843  0.015694  0.012707  0.018620\n",
       "2  0.015164  0.012843  0.004100  0.009474  0.010465  0.012950\n",
       "3  0.017970  0.015694  0.009474  0.004888  0.011487  0.012884\n",
       "4  0.012984  0.012707  0.010465  0.011487  0.003691  0.012606\n",
       "5  0.018714  0.018620  0.012950  0.012884  0.012606  0.006537"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, dot_dist, t3_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "476264ef-20e4-4eb5-a0ed-6613f3115485",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f4e8cdeb-7f38-45f6-8f5f-fff380b49309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 191\n",
      "Min. Dist. = 0.009746947512371186\n",
      "\n",
      "The ability of neural networks to continuously learn and adapt to new tasks\n",
      "while retaining prior knowledge is crucial for many applications. However,\n",
      "current neural networks tend to forget previously learned tasks when trained on\n",
      "new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of\n",
      "Continual Learning (CL) is to alleviate this problem, which is particularly\n",
      "relevant for medical applications, where it may not be feasible to store and\n",
      "access previously used sensitive patient data. In this work, we propose a\n",
      "Continual Learning approach for brain segmentation, where a single network is\n",
      "consecutively trained on samples from different domains. We build upon an\n",
      "importance driven approach and adapt it for medical image segmentation.\n",
      "Particularly, we introduce learning rate regularization to prevent the loss of\n",
      "the network's knowledge. Our results demonstrate that directly restricting the\n",
      "adaptation of important network parameters clearly reduces Catastrophic\n",
      "Forgetting for segmentation across domains.\n",
      "\n",
      "Domain adaptation (DA) has drawn high interests for its capacity to adapt a\n",
      "model trained on labeled source data to perform well on unlabeled or weakly\n",
      "labeled target data from a different domain. Most common DA techniques require\n",
      "the concurrent access to the input images of both the source and target\n",
      "domains. However, in practice, it is common that the source images are not\n",
      "available in the adaptation phase. This is a very frequent DA scenario in\n",
      "medical imaging, for instance, when the source and target images come from\n",
      "different clinical sites. We propose a novel formulation for adapting\n",
      "segmentation networks, which relaxes such a constraint. Our formulation is\n",
      "based on minimizing a label-free entropy loss defined over target-domain data,\n",
      "which we further guide with a domain invariant prior on the segmentation\n",
      "regions. Many priors can be used, derived from anatomical information. Here, a\n",
      "class-ratio prior is learned via an auxiliary network and integrated in the\n",
      "form of a Kullback-Leibler (KL) divergence in our overall loss function. We\n",
      "show the effectiveness of our prior-aware entropy minimization in adapting\n",
      "spine segmentation across different MRI modalities. Our method yields\n",
      "comparable results to several state-of-the-art adaptation techniques, even\n",
      "though is has access to less information, the source images being absent in the\n",
      "adaptation phase. Our straight-forward adaptation strategy only uses one\n",
      "network, contrary to popular adversarial techniques, which cannot perform\n",
      "without the presence of the source images. Our framework can be readily used\n",
      "with various priors and segmentation problems.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "18acf376-d04a-4d14-9b91-46e075786efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([49, 14, 434, 95, 222, 425, 78, 477, 41, 4], dtype='int64')\n",
      "Their Scores: [0.0098, 0.01, 0.0103, 0.0104, 0.0104, 0.0104, 0.0106, 0.0106, 0.0106, 0.0109]\n"
     ]
    }
   ],
   "source": [
    "rftt3 = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b92b8-bbd8-40b9-8702-2ada812b550e",
   "metadata": {},
   "source": [
    "> ## Final Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e0d8b7ec-afc7-4339-a409-122abdcccc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([191, 14, 434, 222, 4, 425, 477, 78, 41, 432], dtype='int64')\n",
      "Int64Index([49, 14, 95, 222, 434, 41, 425, 477, 75, 78], dtype='int64')\n",
      "Int64Index([49, 14, 434, 95, 222, 425, 78, 477, 41, 4], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(rftt1, rftt2, rftt3, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dde15372-e9fb-4c1f-9721-82dea0e01167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14,  41,  78, 222, 425, 434, 477], dtype=int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rftt1,rftt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "92df58cb-914e-4a61-9a51-937518b4e44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  14,  41,  78, 222, 425, 434, 477], dtype=int64)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rftt1,rftt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "33667b24-6b53-446b-8ba7-881372a49ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14,  41,  49,  78,  95, 222, 425, 434, 477], dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(rftt2, rftt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772af90-01b3-45d5-8c8f-0d25c00eb3a6",
   "metadata": {},
   "source": [
    "> Using the same distance metric with the different frequency transformation gave the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19bb8c-9340-4f8e-9444-1d814f000720",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inverse Term Frequency Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "17a9feec-2575-424d-a729-4fceedc25d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(document: str, doc_freq: list, vocab: list, t: Callable) -> Sequence:\n",
    "    bow = np.zeros(len(vocab))\n",
    "    loc = dict(map(lambda pair: (pair[1], pair[0]), enumerate(vocab)))\n",
    "    \n",
    "    for word in nltk.word_tokenize(document):\n",
    "        bow[loc[word.lower()]]+=1\n",
    "    \n",
    "    bow = t(bow)\n",
    "    \n",
    "    for i, v in enumerate(bow):\n",
    "        if v!=0:\n",
    "            bow[i] = np.log((doc_freq[i] + 1) / bow[i])\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b86ad5b3-ecc6-425d-b996-cc8194655884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_mat(N: int, dist: Callable, t: Callable):\n",
    "    bows = [ bag_of_words(get(i), doc_freq, vocab, t) for i in range(N) ]\n",
    "    \n",
    "    sim_mat = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            sim_mat[i,j] = dist(bows[i], bows[j])\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    display(pd.DataFrame(data = sim_mat, columns=range(N), index=range(N)).loc[0:5, 0:5])\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e502723a-3173-41b5-b580-92193dca6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = lambda arr: np.log2(1+arr)\n",
    "t2 = lambda arr: np.log2(1 + t1(arr))\n",
    "t3 = lambda arr, k: (k+1)*arr/(arr+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2aa94a78-5efa-4150-88f4-fa3984522260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_and_frequency(all_docs)->set:\n",
    "    vocab = dict()\n",
    "    for doc in all_docs:\n",
    "        current_words = dict()\n",
    "        for word in nltk.word_tokenize(doc):\n",
    "            try:\n",
    "                current_words[word.lower()]\n",
    "            except:\n",
    "                vocab[ word.lower() ] = vocab.get(word.lower(),  0) + 1\n",
    "                current_words[ word.lower() ] = 1\n",
    "    return vocab.keys(), vocab.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "86d726df-24aa-4ff9-8a46-8b4c0bd438a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, doc_freq = list(map(list, get_vocabulary_and_frequency(df[\"summaries\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d025e0-5f8b-4c3d-af0c-1aae0469c6b9",
   "metadata": {},
   "source": [
    "### First Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ffc1f4fc-b12a-45b7-be28-5199ec656a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.000681  0.001682  0.001410  0.001651  0.001170  0.001588\n",
       "1  0.001682  0.000724  0.001522  0.001649  0.001351  0.001895\n",
       "2  0.001410  0.001522  0.000453  0.000982  0.001021  0.001156\n",
       "3  0.001651  0.001649  0.000982  0.000514  0.001172  0.001080\n",
       "4  0.001170  0.001351  0.001021  0.001172  0.000437  0.001154\n",
       "5  0.001588  0.001895  0.001156  0.001080  0.001154  0.000552"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = get_similarity_mat(500, dot_dist, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8024903d-12d6-4e12-9452-c05c74ca92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ef5a4c7a-1626-43e4-b740-ace20f10943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Dist. Index = 41\n",
      "Min. Dist. = 0.0011564612145292582\n",
      "\n",
      "We generalize a graph-based multiclass semi-supervised classification\n",
      "technique based on diffuse interface methods to multilayer graphs. Besides the\n",
      "treatment of various applications with an inherent multilayer structure, we\n",
      "present a very flexible approach that interprets high-dimensional data in a\n",
      "low-dimensional multilayer graph representation. Highly efficient numerical\n",
      "methods involving the spectral decomposition of the corresponding differential\n",
      "graph operators as well as fast matrix-vector products based on the\n",
      "nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large\n",
      "and high-dimensional data sets. We perform various numerical tests putting a\n",
      "special focus on image segmentation. In particular, we test the performance of\n",
      "our method on data sets with up to 10 million nodes per layer as well as up to\n",
      "104 dimensions resulting in graphs with up to 52 layers. While all presented\n",
      "numerical experiments can be run on an average laptop computer, the linear\n",
      "dependence per iteration step of the runtime on the network size in all stages\n",
      "of our algorithm makes it scalable to even larger and higher-dimensional\n",
      "problems.\n",
      "\n",
      "Automated segmentation in medical image analysis is a challenging task that\n",
      "requires a large amount of manually labeled data. However, most existing\n",
      "learning-based approaches usually suffer from limited manually annotated\n",
      "medical data, which poses a major practical problem for accurate and robust\n",
      "medical image segmentation. In addition, most existing semi-supervised\n",
      "approaches are usually not robust compared with the supervised counterparts,\n",
      "and also lack explicit modeling of geometric structure and semantic\n",
      "information, both of which limit the segmentation accuracy. In this work, we\n",
      "present SimCVD, a simple contrastive distillation framework that significantly\n",
      "advances state-of-the-art voxel-wise representation learning. We first describe\n",
      "an unsupervised training strategy, which takes two views of an input volume and\n",
      "predicts their signed distance maps of object boundaries in a contrastive\n",
      "objective, with only two independent dropout as mask. This simple approach\n",
      "works surprisingly well, performing on the same level as previous fully\n",
      "supervised methods with much less labeled data. We hypothesize that dropout can\n",
      "be viewed as a minimal form of data augmentation and makes the network robust\n",
      "to representation collapse. Then, we propose to perform structural distillation\n",
      "by distilling pair-wise similarities. We evaluate SimCVD on two popular\n",
      "datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\n",
      "dataset. The results on the LA dataset demonstrate that, in two types of\n",
      "labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n",
      "90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\n",
      "previous best results. Our method can be trained in an end-to-end fashion,\n",
      "showing the promise of utilizing SimCVD as a general framework for downstream\n",
      "tasks, such as medical image synthesis and registration.\n"
     ]
    }
   ],
   "source": [
    "get_stats(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "19387376-a915-444c-bcf9-7b274a22e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Candidates: Int64Index([321, 314, 300, 389, 476, 141, 355, 113, 374, 236], dtype='int64')\n",
      "Their Scores: [0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012]\n"
     ]
    }
   ],
   "source": [
    "riftt1 = get_rank(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d1d2-45ad-4d23-8e00-52c2cd3ec246",
   "metadata": {},
   "source": [
    "## Second Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd5bf7-fcdf-432f-aee0-74543a5eebec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
