{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6fc3b253-caee-41e2-b169-05fe699c4d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "from scipy.sparse import dok_matrix, find\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adccb77-61d6-45b4-889e-9aa0e851f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def __get_words(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, parses it's tokens, removing punctuation, stopwords and small words;\n",
    "    It yields each word, one at a time.\n",
    "    \"\"\"\n",
    "    #stopwords = set(map(str.lower, nltk.corpus.stopwords.words(\"english\")))\n",
    "    punctuation = set(string.punctuation)\n",
    "    for word in nltk.tokenize.wordpunct_tokenize(sentence):\n",
    "        word = word.lower()\n",
    "        if (word.isalnum()) \\\n",
    "        and (word not in punctuation):\n",
    "            yield word \n",
    "\n",
    "            \n",
    "def get_vocabulary(documents) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of paragraphs, iterates over it's sentences. \n",
    "    Every time a new word is found, it is added to the dictionary of words with a unique integer reference.\n",
    "    \"\"\"\n",
    "    all_words = {}\n",
    "    #sentences = []\n",
    "    i=0\n",
    "    \n",
    "    for doc in tqdm(documents):\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            for word in __get_words(sentence):\n",
    "                if word not in all_words:\n",
    "                    all_words[ word ] = i\n",
    "                    i+=1                           \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54714911-319a-40e5-9ccb-8f21db7602f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b34151-aba3-4877-9b31-b785d462ed82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:32<00:00, 1575.45it/s]\n"
     ]
    }
   ],
   "source": [
    "words = get_vocabulary(df.summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5730463-2ab8-4b11-a11a-f8b88ff92d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_words = {item[1]:item[0] for item in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c94bb9-f227-463d-a28b-a5e10b6b2721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58933"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92941f21-7189-44f7-b633-5674d9eb2459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_doc(idx):\n",
    "    return df.loc[idx, \"summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c74920-a6aa-40fe-9f0b-d3557f971589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2int(word):\n",
    "    return words[word]\n",
    "\n",
    "def int2word(idx):\n",
    "    return rev_words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b4a6b631-d518-422e-a2b5-f0012115453f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_most_similar_from_buckets(buckets, n_docs):\n",
    "    distances = { i:{} for i in range(n_docs)}\n",
    "    \n",
    "    for key in tqdm(buckets):\n",
    "        bucket = buckets[key]\n",
    "        for pair in permutations(bucket, 2):\n",
    "            distances_to_doc_a = distances[pair[0]]\n",
    "            distances_to_doc_a[pair[1]] = distances_to_doc_a.get(pair[1], 0) + 1\n",
    "            \n",
    "    for key in tqdm(distances.keys()):\n",
    "        docs = list(distances[key].keys())\n",
    "        counts = list(distances[key].values())\n",
    "\n",
    "        order = np.argsort(counts)[::-1]\n",
    "        distances[key] = [docs[i] for i in order]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31412e-5136-402c-8b29-328e7e1566d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LSHT for Jaccard Similarity | Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f1cdcd-b279-4e9d-8426-29a845c654cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bag_of_words(documents, vocabulary):\n",
    "    N = len(documents)\n",
    "    docs = {i:set() for i in range(N)}#dok_matrix((len(documents), len(vocabulary)))\n",
    "    \n",
    "    for i, d in tqdm(enumerate(documents)):\n",
    "        for sentence in nltk.sent_tokenize(d):\n",
    "            for word in __get_words(sentence):\n",
    "                col = vocabulary[word]\n",
    "                docs[i]|= {col}\n",
    "    \n",
    "    sparse_docs = dok_matrix((len(documents), len(vocabulary)))\n",
    "    for row, cols in tqdm(docs.items()):\n",
    "        cols = list(cols)\n",
    "        sparse_docs[row, cols]=1\n",
    "    return sparse_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b5b457-f261-40d0-9433-8112a4d74e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [00:38, 1353.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:17<00:00, 3011.95it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = to_bag_of_words(df.summaries, words).tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644f77f7-e353-4301-a957-d7ca865b7270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51774, 58933)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a7aaa5-bcf0-44d6-9ed8-dd0cd7bb61d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buckets(documents, permutations, N, B, R, NB):\n",
    "    buckets = {}\n",
    "    \n",
    "    docs_set = set(range(N))\n",
    "    \n",
    "    for band in tqdm(range(B)):\n",
    "        signatures = np.zeros((N, R), dtype=int)\n",
    "        for r in range(R):\n",
    "            current_perm = permutations[band*R + r]\n",
    "            L = docs_set.copy()\n",
    "            i=0\n",
    "            while len(L)>0:\n",
    "                elem = current_perm[i]\n",
    "                docs_found = documents[elem] & L\n",
    "                \n",
    "                if len(docs_found)>0:\n",
    "                    signatures[list(docs_found), r] = i\n",
    "                    L -= docs_found\n",
    "                i+=1\n",
    "                if i==N:\n",
    "                    signatures[list(L),r]=i\n",
    "                    L = {}\n",
    "        \n",
    "        for doc in range(N):\n",
    "            bucket = hash(tuple(signatures[doc]))%NB\n",
    "            buckets.setdefault((band, bucket), set()).add(doc)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8369808f-62db-4272-8157-6ce004249c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSHT(documents, B, R, NB=28934501):\n",
    "    N, M = documents.shape\n",
    "    \n",
    "    #d_transpose = documents.T\n",
    "    d_transpose = []\n",
    "    for i in tqdm(range(M)):\n",
    "        d_transpose.append( \n",
    "            set( find( documents[:, i] )[0] )\n",
    "        )\n",
    "    \n",
    "    P = B*R\n",
    "    permutations = np.array([np.random.permutation(M) for _ in range(P)])\n",
    "    buckets = get_buckets(d_transpose, permutations, N, B, R, NB)\n",
    "    return buckets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d4eefa-aab0-4461-be5a-8bffc4988331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58933/58933 [00:23<00:00, 2546.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [10:32<00:00, 15.81s/it]\n"
     ]
    }
   ],
   "source": [
    "buckets = LSHT(docs, 40, 5 , NB=999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea353c0-a8ed-4ad5-92c3-bebc3781c236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1432952"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb4eee51-e9e7-4ab4-a286-3fd9f655f941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1432952/1432952 [00:04<00:00, 330317.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:02<00:00, 21462.43it/s]\n"
     ]
    }
   ],
   "source": [
    "similar = get_most_similar_from_buckets(buckets, docs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0a4d7a-a8b0-4d35-a254-26d7592d3244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17704, 27091, 514, 6225, 9138]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaa06153-e053-4052-92af-43483adb80b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def JaccardSim(d1, d2):\n",
    "    d1, d2 = d1.toarray(), d2.toarray()\n",
    "    \n",
    "    a =np.inner(d1,d2)\n",
    "    bc=np.sum(d1+d2)-a\n",
    "    return a/bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91c1d62b-76aa-4ad7-b50f-4bb360a52690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998732227786"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (1 - 0.8**5)**40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f9267fc-334e-4c6c-a2ef-d6670182401f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]]),\n",
       " array([[0.19565217]]),\n",
       " array([[0.18954248]]),\n",
       " array([[0.17699115]]),\n",
       " array([[0.12222222]])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=10\n",
    "list(map(lambda j: JaccardSim(docs[i], docs[j]), similar[i][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95f3c6f3-099f-413d-ab57-6c39f11535d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4866, 19173, 8492, 37524, 17726]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar[i][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caceab98-1a71-4dad-ba3a-3e332309ef00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning has been utilized to perform tasks in many different domains\n",
      "such as classification, object detection, image segmentation and natural\n",
      "language analysis. Data labeling has always been one of the most important\n",
      "tasks in machine learning. However, labeling large amounts of data increases\n",
      "the monetary cost in machine learning. As a result, researchers started to\n",
      "focus on reducing data annotation and labeling costs. Transfer learning was\n",
      "designed and widely used as an efficient approach that can reasonably reduce\n",
      "the negative impact of limited data, which in turn, reduces the data\n",
      "preparation cost. Even transferring previous knowledge from a source domain\n",
      "reduces the amount of data needed in a target domain. However, large amounts of\n",
      "annotated data are still demanded to build robust models and improve the\n",
      "prediction accuracy of the model. Therefore, researchers started to pay more\n",
      "attention on auto annotation and labeling. In this survey paper, we provide a\n",
      "review of previous techniques that focuses on optimized data annotation and\n",
      "labeling for video, audio, and text data.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cce3596-f7da-49fc-b809-72b4d132d0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning has been utilized to perform tasks in many different domains\n",
      "such as classification, object detection, image segmentation and natural\n",
      "language analysis. Data labeling has always been one of the most important\n",
      "tasks in machine learning. However, labeling large amounts of data increases\n",
      "the monetary cost in machine learning. As a result, researchers started to\n",
      "focus on reducing data annotation and labeling costs. Transfer learning was\n",
      "designed and widely used as an efficient approach that can reasonably reduce\n",
      "the negative impact of limited data, which in turn, reduces the data\n",
      "preparation cost. Even transferring previous knowledge from a source domain\n",
      "reduces the amount of data needed in a target domain. However, large amounts of\n",
      "annotated data are still demanded to build robust models and improve the\n",
      "prediction accuracy of the model. Therefore, researchers started to pay more\n",
      "attention on auto annotation and labeling. In this survey paper, we provide a\n",
      "review of previous techniques that focuses on optimized data annotation and\n",
      "labeling for video, audio, and text data.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(4866))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ee162-6ae2-48c0-b820-2c902dc4b6b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSHT for Cosine Similarity | TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a2423ef-8a21-4243-9331-951f8427943b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_raw_text(documents):\n",
    "    new_docs = []\n",
    "    for i, d in tqdm(enumerate(documents)):\n",
    "        current = []\n",
    "        for sentence in nltk.sent_tokenize(d):\n",
    "            for word in __get_words(sentence):\n",
    "                current.append( word)\n",
    "        new_docs.append(current)\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a594ff2-75d8-49f8-843d-8e06c76d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Counts the ocurrence of each word in the document corpus.\n",
    "    \"\"\"\n",
    "    #return dict(zip(*np.unique(doc, return_counts=True)))\n",
    "    return np.unique(doc, return_counts=True)\n",
    "    \n",
    "def get_tf_matrix(docs, vocab):\n",
    "    N, M = len(docs), len(vocab)\n",
    "    tf_matrix = dok_matrix((N, M))\n",
    "    \n",
    "    for i, doc in tqdm(enumerate(docs)):\n",
    "        #calc document  tf vector\n",
    "        words, counts = count_words(doc)\n",
    "        if len(words)>0:\n",
    "            max_value = counts.max()\n",
    "            \n",
    "            words_idx = list(map(word2int, words))\n",
    "            tf_matrix[i, words_idx] = counts/max_value\n",
    "    return tf_matrix\n",
    "\n",
    "def get_idf_matrix(docs, vocab):\n",
    "    N = len(docs)\n",
    "    word_counts = np.zeros(len(vocab))\n",
    "    \n",
    "    for doc in tqdm(docs):\n",
    "        for word in np.unique(doc):\n",
    "            word_counts[ word2int(word) ] += 1\n",
    "    \n",
    "    return np.log2( (N + 1) / (word_counts + 1) )\n",
    "\n",
    "\n",
    "def get_tf_idf(docs, vocab):\n",
    "    tf = get_tf_matrix(docs, vocab).tocsr()\n",
    "    idf = get_idf_matrix(docs, vocab)\n",
    "    \n",
    "    return tf.multiply(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3872258-0089-4db2-8c7d-aab997b35a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [00:32, 1583.14it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = process_raw_text(df.summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be76d345-3feb-4fff-9c8e-42f58cabc15d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [00:36, 1417.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:16<00:00, 3079.07it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf = get_tf_idf(docs, words).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a12d0f3-67e8-4f94-9af7-1451483f8b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 58933)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = tfidf[:10]\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6258eac9-54cf-45f2-a8fd-b7f1c136e5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 58933)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7983d00c-4d07-4c2f-bbbf-ce77f9b5b1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectors = np.where(np.random.random( (tfidf.shape[1], 5) )<=0.5, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b0261df-9339-4d16-8c2c-fc8fd8ffee3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigs = np.where( (subset @ vectors)<=0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "799473a5-9836-4e4b-93d9-21f7340f19e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4c163fa0-8771-441a-927f-94da41a8c3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins = 2**np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb11f554-5d72-42f3-b192-982aa564a613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 17,  9, 17,  0, 13, 10, 13, 25,  9])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigs @ bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9929c293-e57d-41a0-965a-53ec7a85c2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buckets_cosine(documents, vectors, N, B, R, NB):\n",
    "    buckets = {}\n",
    "    signatures = np.where( (documents @ vectors) <= 0, 0, 1)\n",
    "    binary_power = 2**np.arange(R)\n",
    "    \n",
    "    for band in tqdm(range(B)):        \n",
    "        band_signatures = signatures[:, band*R:band*R+R]\n",
    "        ##print(band_signatures)\n",
    "        #print(band_signatures.shape)\n",
    "        \n",
    "        for doc in range(N):\n",
    "            bucket = hash(tuple(band_signatures[doc]))%NB\n",
    "            buckets.setdefault((band, bucket), set()).add(doc)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fe093124-97c3-41ff-a009-971b51c2bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSHT_cosine(documents, B, R, NB=28934501):\n",
    "    N, M = documents.shape\n",
    "    \n",
    "    P = B*R\n",
    "    v_vectors = np.where(np.random.random(size=(M, P))<=0.5, -1, 1)\n",
    "    buckets = get_buckets_cosine(documents, v_vectors, N, B, R, NB)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "88c9ca14-b7cb-41be-9632-d1271b3f782c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:21<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "buckets = LSHT_cosine(tfidf, 80, 20, NB=999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "de8b340c-239b-49ef-a0cf-5163a19161a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2981805/2981805 [00:03<00:00, 796048.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:00<00:00, 92685.97it/s]\n"
     ]
    }
   ],
   "source": [
    "sims = get_most_similar_from_buckets(buckets, tfidf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "00e5f116-3051-4be8-9091-111507595919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.58497428, 2.16827036, 0.06165452, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[0].toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d64c2d99-f9ec-4690-927d-9f2b332da6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0.026569881752453228,\n",
       " 0.01870539138215488,\n",
       " 0.019666222438966097,\n",
       " 0.011531970680638715,\n",
       " 0.04602095817530805,\n",
       " 0.28984236025492627,\n",
       " 0.28984236025492627]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = randint(0, 51774)\n",
    "list(map(lambda j: 1 - cosine(tfidf[i].toarray().ravel(), tfidf[j].toarray().ravel()), sims[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "75aeaf3f-dc75-4697-86a9-73a6a1d6f928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15718, 20921, 39075, 22102, 39971]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[i][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a6b9486a-47ab-40d2-82fd-5846a8f13fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we study a new representation-learning task, which we termed\n",
      "as disassembling object representations. Given an image featuring multiple\n",
      "objects, the goal of disassembling is to acquire a latent representation, of\n",
      "which each part corresponds to one category of objects. Disassembling thus\n",
      "finds its application in a wide domain such as image editing and few- or\n",
      "zero-shot learning, as it enables category-specific modularity in the learned\n",
      "representations. To this end, we propose an unsupervised approach to achieving\n",
      "disassembling, named Unsupervised Disassembling Object Representation (UDOR).\n",
      "UDOR follows a double auto-encoder architecture, in which a fuzzy\n",
      "classification and an object-removing operation are imposed. The fuzzy\n",
      "classification constrains each part of the latent representation to encode\n",
      "features of up to one object category, while the object-removing, combined with\n",
      "a generative adversarial network, enforces the modularity of the\n",
      "representations and integrity of the reconstructed image. Furthermore, we\n",
      "devise two metrics to respectively measure the modularity of disassembled\n",
      "representations and the visual integrity of reconstructed images. Experimental\n",
      "results demonstrate that the proposed UDOR, despited unsupervised, achieves\n",
      "truly encouraging results on par with those of supervised methods.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "19cf3432-f657-4fda-a1d6-999b6233d84a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem of air pollution threatens public health. Air quality forecasting\n",
      "can provide the air quality index hours or even days later, which can help the\n",
      "public to prevent air pollution in advance. Previous works focus on citywide\n",
      "air quality forecasting and cannot solve nationwide city forecasting problem,\n",
      "whose difficulties lie in capturing the latent dependencies between\n",
      "geographically distant but highly correlated cities. In this paper, we propose\n",
      "the group-aware graph neural network (GAGNN), a hierarchical model for\n",
      "nationwide city air quality forecasting. The model constructs a city graph and\n",
      "a city group graph to model the spatial and latent dependencies between cities,\n",
      "respectively. GAGNN introduces differentiable grouping network to discover the\n",
      "latent dependencies among cities and generate city groups. Based on the\n",
      "generated city groups, a group correlation encoding module is introduced to\n",
      "learn the correlations between them, which can effectively capture the\n",
      "dependencies between city groups. After the graph construction, GAGNN\n",
      "implements message passing mechanism to model the dependencies between cities\n",
      "and city groups. The evaluation experiments on Chinese city air quality dataset\n",
      "indicate that our GAGNN outperforms existing forecasting models.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(20921))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "575b4c79-c691-4b00-aa05-555a31653e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11051"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "eda2cb91-0082-48eb-9c49-5b0fcb71935a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:25<00:00, 1996.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 0.31584664, 0.31584664,\n",
       "       0.31388063, 0.309387  , 0.30180997, 0.29849764, 0.29408726])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = []\n",
    "for j in tqdm(range(tfidf.shape[0])):\n",
    "    similarity.append(\n",
    "        1 - cosine(\n",
    "            tfidf[i].toarray().ravel(), tfidf[j].toarray().ravel()\n",
    "        )\n",
    "    )\n",
    "similarity = np.array(similarity)\n",
    "idx = np.argsort(similarity)[::-1][:10]\n",
    "similarity[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
