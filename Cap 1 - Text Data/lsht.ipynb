{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc3b253-caee-41e2-b169-05fe699c4d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import dok_matrix, find\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adccb77-61d6-45b4-889e-9aa0e851f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def __get_words(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, parses it's tokens, removing punctuation, stopwords and small words;\n",
    "    It yields each word, one at a time.\n",
    "    \"\"\"\n",
    "    #stopwords = set(map(str.lower, nltk.corpus.stopwords.words(\"english\")))\n",
    "    punctuation = set(string.punctuation)\n",
    "    for word in nltk.tokenize.wordpunct_tokenize(sentence):\n",
    "        word = word.lower()\n",
    "        if (word.isalnum()) \\\n",
    "        and (word not in punctuation):\n",
    "            yield word \n",
    "\n",
    "            \n",
    "def get_vocabulary(documents) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of paragraphs, iterates over it's sentences. \n",
    "    Every time a new word is found, it is added to the dictionary of words with a unique integer reference.\n",
    "    \"\"\"\n",
    "    all_words = {}\n",
    "    #sentences = []\n",
    "    i=1\n",
    "    \n",
    "    for doc in tqdm(documents):\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            for word in __get_words(sentence):\n",
    "                if word not in all_words:\n",
    "                    all_words[ word ] = i\n",
    "                    i+=1                           \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54714911-319a-40e5-9ccb-8f21db7602f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b34151-aba3-4877-9b31-b785d462ed82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:19<00:00, 2631.62it/s]\n"
     ]
    }
   ],
   "source": [
    "words = get_vocabulary(df.summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5730463-2ab8-4b11-a11a-f8b88ff92d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_words = {item[1]:item[0] for item in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c94bb9-f227-463d-a28b-a5e10b6b2721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58933"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "92941f21-7189-44f7-b633-5674d9eb2459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_doc(idx):\n",
    "    return df.loc[idx, \"summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c74920-a6aa-40fe-9f0b-d3557f971589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2int(word):\n",
    "    return word[word]\n",
    "\n",
    "def int2word(idx):\n",
    "    return rev_words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f1cdcd-b279-4e9d-8426-29a845c654cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bag_of_words(documents, vocabulary):\n",
    "    docs = dok_matrix((len(documents), len(vocabulary) + 1))\n",
    "    \n",
    "    for i, d in tqdm(enumerate(documents)):\n",
    "        for sentence in nltk.sent_tokenize(d):\n",
    "            for word in __get_words(sentence):\n",
    "                col = vocabulary[word]\n",
    "                docs[i, col] = 1\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b0989d-dfa8-45fd-9d5e-daf91982c011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse_to_set(docs):\n",
    "    new_docs = {\n",
    "        i: set(list(map(lambda key: key[0], doc.keys))) for i, doc in tqdm(enumerate(docs))\n",
    "    }\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b5b457-f261-40d0-9433-8112a4d74e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [02:15, 381.70it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = to_bag_of_words(df.summaries, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fb8fb19-e917-4ee2-81d7-9ea0f260c59e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(find(docs[:, 0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cebe6c7-2de9-4f89-9826-1e9bf1e20ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = docs.tocsc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31412e-5136-402c-8b29-328e7e1566d0",
   "metadata": {},
   "source": [
    "## LSHT for Jaccard Similarrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3a7aaa5-bcf0-44d6-9ed8-dd0cd7bb61d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buckets(documents, permutations, N, B, R, NB):\n",
    "    buckets = {}\n",
    "    \n",
    "    docs_set = set(range(N))\n",
    "    \n",
    "    for band in tqdm(range(B)):\n",
    "        signatures = np.zeros((N, R), dtype=int)\n",
    "        for r in range(R):\n",
    "            current_perm = permutations[band*R + r]\n",
    "            L = docs_set.copy()\n",
    "            i=0\n",
    "            while len(L)>0:\n",
    "                elem = current_perm[i]\n",
    "                docs_found = documents[elem] & L\n",
    "                \n",
    "                if len(docs_found)>0:\n",
    "                    signatures[list(docs_found), r] = i\n",
    "                    L -= docs_found\n",
    "                i+=1\n",
    "                if i==N:\n",
    "                    signatures[list(L),r]=i\n",
    "                    L = {}\n",
    "        \n",
    "        for doc in range(N):\n",
    "            bucket = hash(tuple(signatures[doc]))%NB\n",
    "            buckets.setdefault((band, bucket), set()).add(doc)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8369808f-62db-4272-8157-6ce004249c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSHT(documents, B, R, NB=28934501):\n",
    "    N, M = documents.shape\n",
    "    \n",
    "    #d_transpose = documents.T\n",
    "    d_transpose = []\n",
    "    for i in tqdm(range(M)):\n",
    "        d_transpose.append( \n",
    "            set( find( documents[:, i] )[0] )\n",
    "        )\n",
    "    \n",
    "    P = B*R\n",
    "    permutations = np.array([np.random.permutation(M) for _ in range(P)])\n",
    "    buckets = get_buckets(d_transpose, permutations, N, B, R, NB)\n",
    "    return buckets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1d4eefa-aab0-4461-be5a-8bffc4988331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58934/58934 [00:12<00:00, 4590.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [05:40<00:00,  8.50s/it]\n"
     ]
    }
   ],
   "source": [
    "buckets = LSHT(docs, 40, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "48e90304-5b61-4c12-8f15-e24474e13d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_most_similar_from_buckets(buckets, n_docs):\n",
    "    distances = { i:{} for i in range(n_docs)}\n",
    "    \n",
    "    for key in tqdm(buckets):\n",
    "        bucket = buckets[key]\n",
    "        for pair in permutations(bucket, 2):\n",
    "            distances_to_doc_a = distances[pair[0]]\n",
    "            distances_to_doc_a[pair[1]] = distances_to_doc_a.get(pair[1], 0) + 1\n",
    "            \n",
    "    for key in tqdm(distances.keys()):\n",
    "        docs = list(distances[key].keys())\n",
    "        counts = list(distances[key].values())\n",
    "\n",
    "        order = np.argsort(counts)[::-1]\n",
    "        distances[key] = [docs[i] for i in order]\n",
    "    return distances\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb4eee51-e9e7-4ab4-a286-3fd9f655f941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1442940/1442940 [00:03<00:00, 448592.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:02<00:00, 25566.13it/s]\n"
     ]
    }
   ],
   "source": [
    "similar = get_most_similar_from_buckets(buckets, docs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a0a4d7a-a8b0-4d35-a254-26d7592d3244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27091, 17704, 30014, 13449, 5267, 26780, 27076]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aaa06153-e053-4052-92af-43483adb80b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def JaccardSim(d1, d2):\n",
    "    d1, d2 = d1.toarray(), d2.toarray()\n",
    "    \n",
    "    a =np.inner(d1,d2)\n",
    "    bc=np.sum(d1+d2)-a\n",
    "    return a/bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7f9267fc-334e-4c6c-a2ef-d6670182401f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17177914]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JaccardSim(docs[0], docs[30014])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "caceab98-1a71-4dad-ba3a-3e332309ef00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereo matching is one of the widely used techniques for inferring depth from\n",
      "stereo images owing to its robustness and speed. It has become one of the major\n",
      "topics of research since it finds its applications in autonomous driving,\n",
      "robotic navigation, 3D reconstruction, and many other fields. Finding pixel\n",
      "correspondences in non-textured, occluded and reflective areas is the major\n",
      "challenge in stereo matching. Recent developments have shown that semantic cues\n",
      "from image segmentation can be used to improve the results of stereo matching.\n",
      "Many deep neural network architectures have been proposed to leverage the\n",
      "advantages of semantic segmentation in stereo matching. This paper aims to give\n",
      "a comparison among the state of art networks both in terms of accuracy and in\n",
      "terms of speed which are of higher importance in real-time applications.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7cce3596-f7da-49fc-b809-72b4d132d0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric model fitting is a fundamental research topic in computer vision\n",
      "and it aims to fit and segment multiple-structure data. In this paper, we\n",
      "propose a novel superpixel-guided two-view geometric model fitting method\n",
      "(called SDF), which can obtain reliable and consistent results for real images.\n",
      "Specifically, SDF includes three main parts: a deterministic sampling\n",
      "algorithm, a model hypothesis updating strategy and a novel model selection\n",
      "algorithm. The proposed deterministic sampling algorithm generates a set of\n",
      "initial model hypotheses according to the prior information of superpixels.\n",
      "Then the proposed updating strategy further improves the quality of model\n",
      "hypotheses. After that, by analyzing the properties of the updated model\n",
      "hypotheses, the proposed model selection algorithm extends the conventional\n",
      "\"fit-and-remove\" framework to estimate model instances in multiple-structure\n",
      "data. The three parts are tightly coupled to boost the performance of SDF in\n",
      "both speed and accuracy, and SDF has the deterministic nature. Experimental\n",
      "results show that the proposed SDF has significant advantages over several\n",
      "state-of-the-art fitting methods when it is applied to real images with\n",
      "single-structure and multiple-structure data.\n"
     ]
    }
   ],
   "source": [
    "print(get_doc(30014))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ee162-6ae2-48c0-b820-2c902dc4b6b7",
   "metadata": {},
   "source": [
    "## LHST for Cosine Similarity, using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a594ff2-75d8-49f8-843d-8e06c76d0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
