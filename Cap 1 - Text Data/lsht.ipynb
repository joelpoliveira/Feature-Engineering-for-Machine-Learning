{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fc3b253-caee-41e2-b169-05fe699c4d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint, choices\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "from scipy.sparse import dok_matrix, find\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adccb77-61d6-45b4-889e-9aa0e851f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def __get_words(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, parses it's tokens, removing punctuation, stopwords and small words;\n",
    "    It yields each word, one at a time.\n",
    "    \"\"\"\n",
    "    #stopwords = set(map(str.lower, nltk.corpus.stopwords.words(\"english\")))\n",
    "    punctuation = set(string.punctuation)\n",
    "    for word in nltk.tokenize.wordpunct_tokenize(sentence):\n",
    "        word = word.lower()\n",
    "        if (word.isalnum()) \\\n",
    "        and (word not in punctuation):\n",
    "            yield word \n",
    "\n",
    "            \n",
    "def get_vocabulary(documents) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of paragraphs, iterates over it's sentences. \n",
    "    Every time a new word is found, it is added to the dictionary of words with a unique integer reference.\n",
    "    \"\"\"\n",
    "    all_words = {}\n",
    "    #sentences = []\n",
    "    i=0\n",
    "    \n",
    "    for doc in tqdm(documents):\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            for word in __get_words(sentence):\n",
    "                if word not in all_words:\n",
    "                    all_words[ word ] = i\n",
    "                    i+=1                           \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54714911-319a-40e5-9ccb-8f21db7602f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b34151-aba3-4877-9b31-b785d462ed82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:19<00:00, 2614.45it/s]\n"
     ]
    }
   ],
   "source": [
    "words = get_vocabulary(df.summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5730463-2ab8-4b11-a11a-f8b88ff92d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_words = {item[1]:item[0] for item in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c94bb9-f227-463d-a28b-a5e10b6b2721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58933"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92941f21-7189-44f7-b633-5674d9eb2459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_doc(idx):\n",
    "    return df.loc[idx, \"summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c74920-a6aa-40fe-9f0b-d3557f971589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2int(word):\n",
    "    return words[word]\n",
    "\n",
    "def int2word(idx):\n",
    "    return rev_words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a6b631-d518-422e-a2b5-f0012115453f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_most_similar_from_buckets(buckets, n_docs):\n",
    "    distances = { i:{} for i in range(n_docs)}\n",
    "    \n",
    "    for key in tqdm(buckets):\n",
    "        bucket = buckets[key]\n",
    "        for pair in permutations(bucket, 2):\n",
    "            distances_to_doc_a = distances[pair[0]]\n",
    "            distances_to_doc_a[pair[1]] = distances_to_doc_a.get(pair[1], 0) + 1\n",
    "            \n",
    "    for key in tqdm(distances.keys()):\n",
    "        docs = list(distances[key].keys())\n",
    "        counts = list(distances[key].values())\n",
    "\n",
    "        order = np.argsort(counts)[::-1]\n",
    "        distances[key] = [docs[i] for i in order]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5344c5bf-e8dc-45b0-9d55-5045709f3094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_true_and_false_positive(candidates, similars):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    for doc_i in tqdm(candidates):\n",
    "        true_positive += len(similars[doc_i])\n",
    "        false_positive+= len(candidates[doc_i]) - len(similars[doc_i])\n",
    "    return true_positive, false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31412e-5136-402c-8b29-328e7e1566d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSHT for Jaccard Similarity | Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b7e9d-18f4-489e-963e-7a67b93d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JaccardSim(d1, d2):    \n",
    "    a =np.inner(d1,d2)\n",
    "    bc=np.sum(d1+d2)-a\n",
    "    return a/bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d14a0-0ac6-4d1a-b3fc-91753197e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_similar_from_buckets_jacard(potential_similar_docs, matrix, thresh=0.7):\n",
    "    similar_docs_filtered = {}\n",
    "    for doc_i, docs_j in tqdm(potential_similar_docs.items()):\n",
    "        dists =JaccardSim(\n",
    "                matrix[doc_i].toarray(),\n",
    "                matrix[docs_j].toarray()\n",
    "            ).ravel()\n",
    "        \n",
    "        \n",
    "        candidates = potential_similar_docs[doc_i]\n",
    "        similar_docs_filtered[doc_i] = candidates[dists>=thresh]\n",
    "    return similar_docs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0f1cdcd-b279-4e9d-8426-29a845c654cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bag_of_words(documents, vocabulary):\n",
    "    N = len(documents)\n",
    "    docs = {i:set() for i in range(N)}#dok_matrix((len(documents), len(vocabulary)))\n",
    "    \n",
    "    for i, d in tqdm(enumerate(documents)):\n",
    "        for sentence in nltk.sent_tokenize(d):\n",
    "            for word in __get_words(sentence):\n",
    "                col = vocabulary[word]\n",
    "                docs[i]|= {col}\n",
    "    \n",
    "    sparse_docs = dok_matrix((len(documents), len(vocabulary)))\n",
    "    for row, cols in tqdm(docs.items()):\n",
    "        cols = list(cols)\n",
    "        sparse_docs[row, cols]=1\n",
    "    return sparse_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b4b5b457-f261-40d0-9433-8112a4d74e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [00:22, 2267.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:10<00:00, 5176.09it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = to_bag_of_words(df.summaries, words).tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "644f77f7-e353-4301-a957-d7ca865b7270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51774, 58933)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3a7aaa5-bcf0-44d6-9ed8-dd0cd7bb61d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buckets(documents, permutations, N, B, R, NB):\n",
    "    buckets = {}\n",
    "    \n",
    "    docs_set = set(range(N))\n",
    "    \n",
    "    for band in tqdm(range(B)):\n",
    "        signatures = np.zeros((N, R), dtype=int)\n",
    "        for r in range(R):\n",
    "            current_perm = permutations[band*R + r]\n",
    "            L = docs_set.copy()\n",
    "            i=0\n",
    "            while len(L)>0:\n",
    "                elem = current_perm[i]\n",
    "                docs_found = documents[elem] & L\n",
    "                \n",
    "                if len(docs_found)>0:\n",
    "                    signatures[list(docs_found), r] = i\n",
    "                    L -= docs_found\n",
    "                i+=1\n",
    "                if i==N:\n",
    "                    signatures[list(L),r]=i\n",
    "                    L = {}\n",
    "        \n",
    "        for doc in range(N):\n",
    "            bucket = hash(tuple(signatures[doc]))%NB\n",
    "            buckets.setdefault((band, bucket), set()).add(doc)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8369808f-62db-4272-8157-6ce004249c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSHT(documents, B, R, NB=28934501):\n",
    "    N, M = documents.shape\n",
    "    \n",
    "    #d_transpose = documents.T\n",
    "    d_transpose = []\n",
    "    for i in tqdm(range(M)):\n",
    "        d_transpose.append( \n",
    "            set( find( documents[:, i] )[0] )\n",
    "        )\n",
    "    \n",
    "    P = B*R\n",
    "    permutations = np.array([np.random.permutation(M) for _ in range(P)])\n",
    "    buckets = get_buckets(d_transpose, permutations, N, B, R, NB)\n",
    "    return buckets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432c094-442d-4676-a893-421993418ce5",
   "metadata": {},
   "source": [
    "### Estimate good choices of B and R for high positive rate and low positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a1d4eefa-aab0-4461-be5a-8bffc4988331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58933/58933 [00:10<00:00, 5698.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:10<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "subdocs = docs[choices(range(docs.shape[0]), k=10000)]\n",
    "buckets = LSHT(subdocs, 40, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0ea353c0-a8ed-4ad5-92c3-bebc3781c236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344809"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cb4eee51-e9e7-4ab4-a286-3fd9f655f941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344809/344809 [00:00<00:00, 1174608.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 149284.74it/s]\n"
     ]
    }
   ],
   "source": [
    "similar = get_most_similar_from_buckets(buckets, subdocs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a5f67223-3419-4b36-8ab8-600e617cabb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 842229.72it/s]\n"
     ]
    }
   ],
   "source": [
    "similar = { doc_i:np.array(docs_j) for doc_i, docs_j in tqdm(similar.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2a0a4d7a-a8b0-4d35-a254-26d7592d3244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a713d9b5-ebc0-4dfb-8369-25105969633e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:37<00:00, 102.63it/s]\n"
     ]
    }
   ],
   "source": [
    "true_sim = filter_similar_from_buckets_jacard(similar, subdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b0684e59-34a0-4f23-87c5-cbe6b9a186a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1607752.22it/s]\n"
     ]
    }
   ],
   "source": [
    "t,f = count_true_and_false_positive(similar, true_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "aad95836-71ef-4d36-84e8-b044a8cd7593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6469512195121951"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t/(t+f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ee162-6ae2-48c0-b820-2c902dc4b6b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSHT for Cosine Similarity | TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dcb018bc-567a-4322-9170-c256ad5ce72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_similar_from_buckets(potential_similar_docs, matrix, thresh=0.7):\n",
    "    similar_docs_filtered = {}\n",
    "    for doc_i, docs_j in tqdm(potential_similar_docs.items()):\n",
    "        dists = 1 - np.array([\n",
    "            cosine(\n",
    "                matrix[doc_i].toarray().ravel(),\n",
    "                matrix[doc_j].toarray().ravel()\n",
    "            ) for doc_j in docs_j\n",
    "        ])\n",
    "        \n",
    "        candidates = potential_similar_docs[doc_i]\n",
    "        similar_docs_filtered[doc_i] = candidates[dists>=thresh]\n",
    "    return similar_docs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2423ef-8a21-4243-9331-951f8427943b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_raw_text(documents):\n",
    "    new_docs = []\n",
    "    for i, d in tqdm(enumerate(documents)):\n",
    "        current = []\n",
    "        for sentence in nltk.sent_tokenize(d):\n",
    "            for word in __get_words(sentence):\n",
    "                current.append( word)\n",
    "        new_docs.append(current)\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a594ff2-75d8-49f8-843d-8e06c76d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Counts the ocurrence of each word in the document corpus.\n",
    "    \"\"\"\n",
    "    #return dict(zip(*np.unique(doc, return_counts=True)))\n",
    "    return np.unique(doc, return_counts=True)\n",
    "    \n",
    "def get_tf_matrix(docs, vocab):\n",
    "    N, M = len(docs), len(vocab)\n",
    "    tf_matrix = dok_matrix((N, M))\n",
    "    \n",
    "    for i, doc in tqdm(enumerate(docs)):\n",
    "        #calc document  tf vector\n",
    "        words, counts = count_words(doc)\n",
    "        if len(words)>0:\n",
    "            max_value = counts.max()\n",
    "            \n",
    "            words_idx = list(map(word2int, words))\n",
    "            tf_matrix[i, words_idx] = counts/max_value\n",
    "    return tf_matrix\n",
    "\n",
    "def get_idf_matrix(docs, vocab):\n",
    "    N = len(docs)\n",
    "    word_counts = np.zeros(len(vocab))\n",
    "    \n",
    "    for doc in tqdm(docs):\n",
    "        for word in np.unique(doc):\n",
    "            word_counts[ word2int(word) ] += 1\n",
    "    \n",
    "    return np.log2( (N + 1) / (word_counts + 1) )\n",
    "\n",
    "\n",
    "def get_tf_idf(docs, vocab):\n",
    "    tf = get_tf_matrix(docs, vocab).tocsr()\n",
    "    idf = get_idf_matrix(docs, vocab)\n",
    "    \n",
    "    return tf.multiply(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d3872258-0089-4db2-8c7d-aab997b35a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51774it [00:21, 2442.88it/s]\n",
      "51774it [00:23, 2234.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51774/51774 [00:09<00:00, 5500.77it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = process_raw_text(df.summaries)\n",
    "tfidf = get_tf_idf(docs, words).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab18874-a136-4113-a47b-cf04354084f8",
   "metadata": {},
   "source": [
    "### Estimate good B and R parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "be76d345-3feb-4fff-9c8e-42f58cabc15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subtf = tfidf[ choices(range(tfidf.shape[0]), k=10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9929c293-e57d-41a0-965a-53ec7a85c2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buckets_cosine(documents, vectors, N, B, R, NB):\n",
    "    buckets = {}\n",
    "    signatures = np.where( (documents @ vectors) <= 0, 0, 1)\n",
    "    binary_power = 2**np.arange(R)\n",
    "    \n",
    "    for band in tqdm(range(B)):        \n",
    "        band_signatures = signatures[:, band*R:band*R+R]\n",
    "        ##print(band_signatures)\n",
    "        #print(band_signatures.shape)\n",
    "        \n",
    "        for doc in range(N):\n",
    "            bucket = hash(tuple(band_signatures[doc]))%NB\n",
    "            buckets.setdefault((band, bucket), set()).add(doc)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe093124-97c3-41ff-a009-971b51c2bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSHT_cosine(documents, B, R, NB=28934501):\n",
    "    N, M = documents.shape\n",
    "    \n",
    "    P = B*R\n",
    "    v_vectors = np.where(np.random.random(size=(M, P))<=0.5, -1, 1)\n",
    "    buckets = get_buckets_cosine(documents, v_vectors, N, B, R, NB)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "88c9ca14-b7cb-41be-9632-d1271b3f782c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 26.91it/s]\n"
     ]
    }
   ],
   "source": [
    "buckets = LSHT_cosine(subtf, 40, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "de8b340c-239b-49ef-a0cf-5163a19161a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344768/344768 [00:00<00:00, 1258787.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 151513.00it/s]\n"
     ]
    }
   ],
   "source": [
    "sims = get_most_similar_from_buckets(buckets, subtf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dc7e083b-56e6-41c8-b0a9-b197f8d47e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 909196.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sims = { doc_i:np.array(docs_j) for doc_i, docs_j in tqdm(sims.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "31b8d12e-ef3c-4278-894c-0aefbee3a323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d64c2d99-f9ec-4690-927d-9f2b332da6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2520.21it/s]\n"
     ]
    }
   ],
   "source": [
    "true_sims = filter_similar_from_buckets(sims, subtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a6b9486a-47ab-40d2-82fd-5846a8f13fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1657434.60it/s]\n"
     ]
    }
   ],
   "source": [
    "t,f =count_true_and_false_positive(sims, true_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "19cf3432-f657-4fda-a1d6-999b6233d84a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540722596448254"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t/(t+f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
